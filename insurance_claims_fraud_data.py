# -*- coding: utf-8 -*-
"""Insurance Claims Fraud Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wp4PqQNMM6Hr36C8HGIsGALA2Lg4nB_X
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

insurance=pd.read_csv("/content/drive/MyDrive/Dataset/insurance_data.csv")
employee=pd.read_csv("/content/drive/MyDrive/Dataset/employee_data.csv")
vendor=pd.read_csv("/content/drive/MyDrive/Dataset/vendor_data.csv")

insurance.info()

insurance.head()

employee.info()

employee.head()

vendor.head()

vendor.info()

insurance_employee= employee.merge(insurance, how= 'outer', on= 'AGENT_ID')
insurance_employee.head()

insurance_employee.rename(columns={'ADDRESS_LINE1_x': 'CUSTOMER_ADDRESS_LINE1', 'ADDRESS_LINE2_x': 'CUSTOMER_ADDRESS_LINE2',
                                   'CITY_x': 'CUSTOMER_CITY', 'STATE_x': 'CUSTOMER_STATE', 'POSTAL_CODE_x': 'CUSTOMER_POSTAL_CODE',
                                   'ADDRESS_LINE1_y': 'AGENT_ADDRESS_LINE1', 'ADDRESS_LINE2_y': 'AGENT_ADDRESS_LINE2',
                                   'CITY_y': 'AGENT_CITY', 'STATE_y': 'AGENT_STATE', 'POSTAL_CODE_y': 'AGENT_POSTAL_CODE'},
                          inplace=True)

insurance_employee.columns.tolist()

df = insurance_employee.merge(vendor, how= 'left', on= 'VENDOR_ID')

df.dtypes

df.to_csv('Insurance.csv',index=False)

#checking null values in dataset
df.isnull().sum()

#checking for duplicate rows
df.duplicated().sum()

#droping duplicates if any
df.drop_duplicates()

print("Size of Dataset: {} rows , {} columns".format(df.shape[0],df.shape[1]))

df.rename(columns={'ADDRESS_LINE1': 'VENDOR_ADDRESS_LINE1', 'ADDRESS_LINE2': 'VENDOR_ADDRESS_LINE2',
                     'CITY': 'VENDOR_CITY', 'STATE': 'VENDOR_STATE', 'POSTAL_CODE': 'VENDOR_POSTAL_CODE'},
            inplace=True)

df.columns.tolist()

df.head(10)

df.info()

#A means "Approved" and D means "Denied"
df.CLAIM_STATUS.unique()

df['INSURANCE_TYPE'].value_counts().sort_values()

#to get top three values
df['INSURANCE_TYPE'].value_counts().sort_values(ascending=False)[:3]

df['CLAIM_STATUS'].value_counts()

df.RISK_SEGMENTATION.unique()

#dropping unnecessary columns
df=df.drop(['AGENT_NAME','AGENT_ID', 'TRANSACTION_ID', 'EMP_ROUTING_NUMBER', 'CUSTOMER_NAME','DATE_OF_JOINING',
            'ACCT_NUMBER', 'TXN_DATE_TIME','EMP_ACCT_NUMBER','POLICY_NUMBER', 'POLICY_EFF_DT', 'LOSS_DT', 'REPORT_DT',
            'VENDOR_NAME','CUSTOMER_ID',"CUSTOMER_ADDRESS_LINE1","CUSTOMER_ADDRESS_LINE2","CUSTOMER_CITY",
            "CUSTOMER_STATE", 'VENDOR_ID',"CUSTOMER_POSTAL_CODE","AGENT_ADDRESS_LINE1","AGENT_ADDRESS_LINE2",
            "AGENT_CITY","AGENT_STATE","AGENT_POSTAL_CODE", "INCIDENT_STATE","INCIDENT_CITY","VENDOR_ADDRESS_LINE1",
            'ROUTING_NUMBER', 'SSN',"VENDOR_ADDRESS_LINE2","VENDOR_CITY","VENDOR_STATE","VENDOR_POSTAL_CODE"],axis=1)

df.nunique()

#histogram
df["RISK_SEGMENTATION"].hist(bins=40,figsize=(5,5))
plt.show()

df.corr()

#correlation heatmap
plt.figure(figsize=(15,10))
sns.heatmap(df.corr(),annot=True)

#checking for missing values

def missing (df):
    missing_number = df.isnull().sum().sort_values(ascending=False)
    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return missing_values

missing(df)

#Suspicious_df = df[(df['CLAIM_STATUS'] == 'A') & (df['RISK_SEGMENTATION'] == 'H') & (df['INCIDENT_SEVERITY'] == 'Major Loss')]
#Suspicious_df.describe()

df.nunique()

#the rows with null values are droped
#df.dropna(axis=0,inplace=True)

df.info()

df.columns

new_cols = [ 'INSURANCE_TYPE', 'PREMIUM_AMOUNT', 'CLAIM_AMOUNT', 'MARITAL_STATUS', 'AGE', 'TENURE', 'EMPLOYMENT_STATUS','NO_OF_FAMILY_MEMBERS', 'RISK_SEGMENTATION', 'HOUSE_TYPE','SOCIAL_CLASS', 'CUSTOMER_EDUCATION_LEVEL', 'INCIDENT_SEVERITY','AUTHORITY_CONTACTED', 'ANY_INJURY', 'POLICE_REPORT_AVAILABLE','INCIDENT_HOUR_OF_THE_DAY','CLAIM_STATUS']
df=df.reindex(columns=new_cols)
df.info()

df.groupby(["RISK_SEGMENTATION", "CLAIM_STATUS"]).size()

df

df.nunique()

#df=pd.get_dummies(df)
#df.info()
  #<class 'pandas.core.frame.DataFrame'>
  #Int64Index: 10000 entries, 0 to 9999
  #Columns: 58374 entries, EMP_ROUTING_NUMBER to CLAIM_STATUS_D
  #dtypes: float64(1), int64(9), uint8(58364)
  #memory usage: 557.4 MB

#x-independent features y-dependent features
x=df.iloc[:,:-1]
y=df.iloc[:,-1].values

x

x=pd.get_dummies(x)

x

x.shape

x.info()

#x=x.drop(['MARITAL_STATUS_N','EMPLOYMENT_STATUS_Y'],axis=1)

x=x.values
x

y

plt.hist(y)
plt.show()

#LabelEncoder
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y)

y

from imblearn.over_sampling import SMOTE
oversample = SMOTE()

#x,y=oversampling.fit_resample(x,y)
df['CLAIM_STATUS'].value_counts()

from sklearn.model_selection import train_test_split
#x_train,x_test,y_train,y_test=train_test_split(x,y,stratify=y,train_size=0.8,random_state=0)

x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.3, random_state=0, shuffle=True, stratify=y)

from imblearn.over_sampling import SMOTE
oversample = SMOTE(random_state=0)
x_train_smote, y_train_smote = oversample.fit_resample(x_train, y_train)

x_train_smote

y_train=y_train_smote

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train_smote)
x_test = sc.transform(x_test) 
print(x_train) 
print(x_test)

x_train.shape

y_train.shape

plt.hist(["x_train_smote","y_train_smote"])
plt.show()

from sklearn.model_selection import cross_val_score, KFold
cv=KFold(n_splits=5,random_state=0,shuffle=True)

# LogisticRegression
from sklearn.linear_model import LogisticRegression
lrModel = LogisticRegression()
lrModel.fit(x_train,y_train_smote)
scores = cross_val_score(lrModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Logistics Regression Average CV Score: ",scores.mean())
#y_pred = lrModel.predict(x_test)

# KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
KnModel = KNeighborsClassifier()
scores = cross_val_score(KnModel, x_train, y_train, cv = cv,scoring='accuracy')
print("KNeighbors Average CV Score: ",scores.mean())

# RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
RfModel = RandomForestClassifier()
scores = cross_val_score(RfModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Random Forest Average CV Score: ",scores.mean())

# DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier
DtModel = DecisionTreeClassifier()
scores = cross_val_score(DtModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Decision Tree Average CV Score: ",scores.mean())

# SVC
from sklearn.svm import SVC
SvmModel = SVC()
scores = cross_val_score(SvmModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Support Vector Machine Average CV Score: ",scores.mean())

# xgb
import xgboost as xgb
XgbModel = xgb.XGBClassifier()
scores = cross_val_score(XgbModel, x_train, y_train, cv = cv,scoring='accuracy')
print("XGBoost Average CV Score: ",scores.mean())

# BaggingClassifier
from sklearn.ensemble import BaggingClassifier
BcModel = BaggingClassifier()
scores = cross_val_score(BcModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Bagging Classifier Average CV Score: ",scores.mean())

# AdaBoostClassifier
from sklearn.ensemble import AdaBoostClassifier
AdbModel = AdaBoostClassifier()
scores = cross_val_score(AdbModel, x_train, y_train, cv = cv,scoring='accuracy')
print("AdaBoost Tree Average CV Score: ",scores.mean())

# Create an Isolation Forest classifier
from sklearn.ensemble import IsolationForest
IslModel= IsolationForest()

# Create an Isolation Forest classifier
IslModel= IsolationForest()
scores = cross_val_score(IslModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Isolation Forest Average CV Score: ",scores.mean())

from sklearn.feature_selection import SelectKBest, f_classif
# Select the top 15 features using ANOVA
selector = SelectKBest(f_classif, k=15)
X_new = selector.fit_transform(x, y)
x=X_new
# Print the scores and p-values for each feature
scores = selector.scores_
pvalues = selector.pvalues_
for i in range(len(scores)):
    print(f"Feature {i+1}: score = {scores[i]:.2f}, p-value = {pvalues[i]:.2f}")
# Print the indices of the selected features
selected_indices = selector.get_support(indices=True)
print(f"Selected feature indices: {selected_indices}")

"""**AFTER** **ANOVA**"""

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score

lrModel = LogisticRegression()
lrModel.fit(x_train,y_train_smote)
scores = cross_val_score(lrModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Logistics Regression Average CV Score: ",scores.mean())
y_pred = lrModel.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(2,2))
sns.heatmap(cm, annot=True, linewidths=1, square = True, cmap = 'Blues_r')
plt.show()
print("\nAccuracy after smote : ",accuracy_score(y_test,y_pred))
print("\nF1 score after smote : ",f1_score(y_test,y_pred))

from sklearn.neighbors import KNeighborsClassifier
KnModel = KNeighborsClassifier()
scores = cross_val_score(KnModel, x_train, y_train, cv = cv,scoring='accuracy')
print("KNeighbors Average CV Score: ",scores.mean())

from sklearn.ensemble import RandomForestClassifier
RfModel = RandomForestClassifier()
scores = cross_val_score(RfModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Random Forest Average CV Score: ",scores.mean())

from sklearn.tree import DecisionTreeClassifier
DtModel = DecisionTreeClassifier()
scores = cross_val_score(DtModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Decision Tree Average CV Score: ",scores.mean())

from sklearn.svm import SVC
SvmModel = SVC()
scores = cross_val_score(SvmModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Support Vector Machine Average CV Score: ",scores.mean())

import xgboost as xgb
XgbModel = xgb.XGBClassifier()
scores = cross_val_score(XgbModel, x_train, y_train, cv = cv,scoring='accuracy')
print("XGBoost Average CV Score: ",scores.mean())

from sklearn.ensemble import BaggingClassifier
BcModel = BaggingClassifier()
scores = cross_val_score(BcModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Bagging Classifier Average CV Score: ",scores.mean())

from sklearn.ensemble import AdaBoostClassifier
AdbModel = AdaBoostClassifier()
scores = cross_val_score(AdbModel, x_train, y_train, cv = cv,scoring='accuracy')
print("AdaBoost Tree Average CV Score: ",scores.mean())

# Create an Isolation Forest classifier
IslModel= IsolationForest()
scores = cross_val_score(IslModel, x_train, y_train, cv = cv,scoring='accuracy')
print("Isolation Forest Average CV Score: ",scores.mean())

KnModel.fit(x_train,y_train)
prediction = KnModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

RfModel.fit(x_train,y_train)
prediction = RfModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

DtModel.fit(x_train,y_train)
prediction = DtModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

lrModel.fit(x_train,y_train)
prediction = lrModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

SvmModel.fit(x_train,y_train)
prediction = SvmModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

XgbModel.fit(x_train,y_train)
prediction = XgbModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

BcModel.fit(x_train,y_train)
prediction = BcModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

AdbModel.fit(x_train,y_train)
prediction = AdbModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The precision Score :", precision_score(prediction,y_test))
print("The recall Score :", recall_score(prediction,y_test))
print("The f1 score :", f1_score(prediction,y_test))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

IslModel.fit(x_train,y_train)
prediction = IslModel.predict(x_test)
print("The Accuracy Score on test data:", accuracy_score(prediction,y_test))
print("The Precision Score : ",precision_score(prediction,y_test,average='micro'))
print("The Recall Score : ",recall_score(prediction,y_test,average='micro'))
print("The f1 score :", f1_score(prediction,y_test,average='micro'))
print("The Classification report : \n",classification_report(prediction,y_test))
sns.heatmap(confusion_matrix(prediction,y_test), annot = True)
plt.show()

svc1=SVC(kernel="rbf",C=100,gamma=0.001,random_state=0)
svc1.fit(x_train,y_train)
y_pred_svc1 = svc1.predict(x_test)
from sklearn import metrics 
cm_svc1=metrics.confusion_matrix(y_pred_svc1,y_test)
plt.figure(figsize=(2,2))
sns.heatmap(cm_svc1, annot=True, linewidths=1, square = True, cmap = 'Blues_r')
plt.show()
print("\nAccuracy after smote : ",accuracy_score(y_pred_svc1,y_test))
print("\nF1 score after smote : ",f1_score(y_pred_svc1,y_test))

from sklearn.model_selection import GridSearchCV

# define the AdaBoost classifier
adaboost = AdaBoostClassifier()

# define the hyperparameters to tune
param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'learning_rate': [0.1, 0.5, 1.0, 2.0]
}

# define the GridSearchCV object
grid_search = GridSearchCV(adaboost, param_grid, cv=5)

# fit the GridSearchCV object to the data
grid_search.fit(x_train,y_train)

# print the best hyperparameters and the corresponding score
print("Best parameters: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)

